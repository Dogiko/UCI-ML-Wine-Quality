{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def array_sign(array):\n",
    "    # return +1, 0, -1 respect to positive, zero, negtive\n",
    "    return 1.*(array>0) - 1.*(array<0)\n",
    "\n",
    "def column_operate(matrix, threshold = 0.00001):\n",
    "    rm = np.array(matrix) # reduced matrix\n",
    "    fm = np.array(matrix) # filtered matrix\n",
    "    ms = matrix.shape # matrix size\n",
    "    mk = np.ones(matrix.shape) # mask\n",
    "    pv = -1*np.ones((ms[1]), dtype = np.int) # pivots\n",
    "    for t in range(ms[1]):\n",
    "        fm = rm*mk # filtered matrix\n",
    "        if np.abs(fm).max() < threshold:\n",
    "            break\n",
    "        \n",
    "        pr, pc = np.unravel_index(np.abs(fm).argmax(), ms) # pivot row, pivot column\n",
    "        rm[:,pc] /= rm[pr][pc]\n",
    "        multi = np.array(rm[pr])\n",
    "        multi[pc] = 0.\n",
    "        rm -= np.dot(rm[:,pc].reshape((ms[0], 1)), multi.reshape((1, ms[1])))\n",
    "        mk[pr] = 0.\n",
    "        mk[:,pc] = 0.\n",
    "        pv[pc] = pr\n",
    "    \n",
    "    rm = rm[:, pv != -1]\n",
    "    pv = pv[pv != -1]\n",
    "    \n",
    "    return rm, pv\n",
    "\n",
    "def mcmc_normal(targets, drop_t = 10, mean=0., std=1.):\n",
    "    output = np.random.normal(mean, std, targets.shape[1:])\n",
    "    if drop_t>1:\n",
    "        for t in range(1, drop_t):\n",
    "            c = np.random.normal(mean, std/np.sqrt(targets[0].size), targets.shape[1:]) # candicate\n",
    "            cd = np.sqrt(np.square(np.subtract(targets, c)).sum(axis=tuple(np.arange(1,len(targets.shape)))).min())\n",
    "            # distance of candicate to target\n",
    "            od = np.sqrt(np.square(np.subtract(targets, output)).sum(axis=tuple(np.arange(1,len(targets.shape)))).min())\n",
    "            # distance of currently output to target\n",
    "            if np.random.rand()*od < cd:\n",
    "                output = np.array(c)\n",
    "    \n",
    "    return output\n",
    "\n",
    "class VariableArray():\n",
    "    def __init__(self, size, cs_initial=0.1):\n",
    "        self.v = np.random.normal(0., 1., size) # array values\n",
    "        self.td = np.zeros(self.v.shape) # total derivative, used to descent\n",
    "        self.ltd = None # last total derivative\n",
    "        self.m = np.zeros(self.v.shape) # moving array\n",
    "        self.cs = cs_initial*np.ones(self.v.shape) # component-wise step\n",
    "        self.work = np.ones(self.v.shape) # working components, defult to be fully connected\n",
    "    \n",
    "    def assign_values(self, values, cs_initial=0.1):\n",
    "        self.v = np.array(values)\n",
    "        self.td = np.zeros(self.v.shape)\n",
    "        self.ltd = None\n",
    "        self.m = np.zeros(self.v.shape)\n",
    "        self.cs = cs_initial*np.ones(self.v.shape)\n",
    "        self.work = np.ones(self.v.shape)\n",
    "    \n",
    "    def derivative_assign(self, values):\n",
    "        if values.shape != self.td.shape:\n",
    "            raise ValueError(\"values shape error\")\n",
    "        \n",
    "        self.ltd = np.array(self.td)\n",
    "        self.td = np.array(values)\n",
    "    \n",
    "    def add_row(self, new_row, cs_initial=0.1):\n",
    "        self.v = np.append(self.v, np.array([new_row]), axis = 0)\n",
    "        self.td = np.append(self.td, np.zeros((1,)+new_row.shape), axis = 0)\n",
    "        self.ltd = None\n",
    "        self.m = np.zeros(self.v.shape)\n",
    "        self.cs = np.append(self.cs, cs_initial*np.ones((1,)+new_row.shape), axis = 0)\n",
    "        self.work = np.ones(self.v.shape)\n",
    "    \n",
    "    def add_column(self, new_column, cs_initial=0.1):\n",
    "        self.v = np.append(self.v, np.array([new_column]).T, axis = 1)\n",
    "        self.td = np.append(self.td, np.zeros(new_column.shape + (1,)), axis = 1)\n",
    "        self.ltd = None\n",
    "        self.m = np.zeros(self.v.shape)\n",
    "        self.cs = np.append(self.cs, cs_initial*np.ones(new_column.shape + (1,)), axis = 1)\n",
    "        self.work = np.ones(self.v.shape)\n",
    "    \n",
    "    def max_cs(self):\n",
    "        return self.cs.max()\n",
    "    \n",
    "    def reset_cs(self, new_cs):\n",
    "        self.cs = new_cs*np.ones(self.cs.shape)\n",
    "    \n",
    "    def descent(self, step = 1., descent_method = \"normal\", regularizer = (\"None\",), td_max = 0.1, move_max=1., move_min=0.000001):\n",
    "        if regularizer[0] == \"r_square\":\n",
    "            self.td += regularizer[1] * self.v\n",
    "        \n",
    "        if regularizer[0] == \"rs_extend\":\n",
    "            self.td += regularizer[1] * ((self.v>regularizer[2])*(self.v-regularizer[2]) + (self.v < -regularizer[2])*(self.v+regularizer[2]))\n",
    "        \n",
    "        if descent_method == \"normal\":\n",
    "            self.m = self.td * (np.abs(self.td) < td_max) + array_sign(self.td)*(np.abs(self.td) >= td_max)\n",
    "            self.v -= step * self.m * self.work\n",
    "        elif descent_method == \"Rprop\":\n",
    "            self.m = array_sign(self.td)\n",
    "            self.cs *= 1.2*(self.td*self.ltd>0) + 1.*(self.td*self.ltd==0) + 0.5*(self.td*self.ltd<0)\n",
    "            self.cs = self.cs * (self.cs < move_max) * (self.cs > move_min)+ move_max*(self.cs >= move_max) + move_min*(self.cs <= move_min)\n",
    "            self.v -= self.cs * self.m * self.work\n",
    "        elif descent_method == \"Dogiko Rprop\":\n",
    "            self.m = array_sign(self.td)\n",
    "            step_change = 1.2*(self.td*self.ltd>0.) + 1.*(self.td*self.ltd==0.) + 1.*(self.td==self.ltd)\n",
    "            step_change[step_change == 0.] = self.td[step_change == 0.]/(self.ltd-self.td)[step_change == 0.]\n",
    "            step_change[step_change < 0.1] = 0.1\n",
    "            self.cs *= step_change\n",
    "            self.cs = self.cs * (self.cs < move_max) * (self.cs > move_min)+ move_max*(self.cs >= move_max) + move_min*(self.cs <= move_min)\n",
    "            self.v -= self.cs * self.m * self.work\n",
    "\n",
    "# Activation functions defined start\n",
    "\n",
    "class Identity():\n",
    "    def trans(self, x):\n",
    "        return x\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return np.ones(x.shape, dtype = np.float64)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Sigmoid():\n",
    "    def trans(self, x):\n",
    "        return expit(x)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return expit(x)*expit(-x)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Hypertan():\n",
    "    def trans(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1. / np.square(np.cosh(x))\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class SoftSign():\n",
    "    def trans(self, x):\n",
    "        return array_sign(x)*(1. - 1./(np.abs(x) + 1.))\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1. / np.square(np.abs(x) + 1.)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Relu():\n",
    "    def trans(self, x):\n",
    "        return x*(x>0)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1.*(x>0)\n",
    "    \n",
    "    def backward(self, x, _input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class LeakyRelu():\n",
    "    def __init__(self, alpha = 0.1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def trans(self, x):\n",
    "        return x*(x>0) + self.alpha*x*(x<0)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1.*(x>0) + self.alpha*(x<0)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class SoftPlus():\n",
    "    def trans(self, x):\n",
    "        return np.log(1. + np.exp(x))\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return expit(x)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Selu():\n",
    "    def __init__(self):\n",
    "        self.ahpha = 1.05071\n",
    "        self.beta = 1.67326\n",
    "    \n",
    "    def trans(self, x):\n",
    "        return self.ahpha*(x*(x>=0) + self.beta*(np.exp(x) - 1)*(x<0))\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return self.ahpha*(1.*(x>=0) + self.beta*np.exp(x)*(x<0))\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Softmax():\n",
    "    def trans(self, x):\n",
    "        output = x - x.max(axis=0)\n",
    "        output = np.exp(output)\n",
    "        output /= output.sum(axis=0)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, x, _input):\n",
    "        tr = self.trans(x) # result of self.trans\n",
    "        return tr*_input - tr*((tr*_input).sum(axis=0))\n",
    "\n",
    "# Activation functions defined end\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, neuron_n, activation_function):\n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        self.nn = neuron_n\n",
    "        self.af = activation_function\n",
    "        self.w = VariableArray((self.nn, 0)) # linear weights working before active function\n",
    "        self.b = VariableArray((self.nn, 1)) # bias working before active function\n",
    "        self.x = np.zeros((0, self.nn))\n",
    "        self.y = np.zeros((0, self.nn))\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        self.x = np.dot(self.w.v, _input) + self.b.v\n",
    "        self.y = self.af.trans(self.x)\n",
    "    \n",
    "    def backward(self, _input, source):\n",
    "        derivative = self.af.backward(self.x, _input)\n",
    "        self.w.derivative_assign(np.dot(derivative, source.T))\n",
    "        self.b.derivative_assign(np.sum(derivative, axis=1).reshape(derivative.shape[0], 1))\n",
    "        derivative = np.dot(derivative.T, self.w.v)\n",
    "        return derivative.T\n",
    "    \n",
    "    def descent(self, step, descent_method, regularizer):\n",
    "        self.w.descent(step, descent_method, regularizer)\n",
    "        self.b.descent(step, descent_method, regularizer)\n",
    "    \n",
    "    def reset_cs(self, new_cs):\n",
    "        self.w.reset_cs(new_cs)\n",
    "        self.b.reset_cs(new_cs)\n",
    "    \n",
    "    def dimension(self):\n",
    "        return self.w.v.size + self.b.v.size\n",
    "\n",
    "class DogikoLearn():\n",
    "    def __init__(self, loss_function = \"r2\"):\n",
    "        self.lf = loss_function # loss function type\n",
    "        self.ly = [] # layers list\n",
    "        self.rg = (\"None\",) # Regularizetion method\n",
    "        self.csi = 0.1 # initial component-wise step when claim new weights and bias\n",
    "    \n",
    "    def r_square_regularizer(self, alpha):\n",
    "        # Assign regularization method as radius square\n",
    "        # i.e Error += alpha*0.5*sum(weight**2) when descent\n",
    "        if alpha <= 0:\n",
    "            raise ValueError(\"Input should be positive\")\n",
    "        \n",
    "        self.rg = (\"r_square\", alpha)\n",
    "    \n",
    "    def rs_extend_regularizer(self, alpha, beta):\n",
    "        # Assign regularization method as radius square\n",
    "        # i.e Error += alpha*0.5sum(weight**2) when descent\n",
    "        if (alpha <= 0) or (alpha <= 0):\n",
    "            raise ValueError(\"All input should be positive\")\n",
    "        \n",
    "        self.rg = (\"rs_extend\", alpha, beta)\n",
    "    \n",
    "    def set_training_data(self, training_input, training_labels):\n",
    "        self.tx = np.array(training_input) # training data input\n",
    "        self.ty = np.array(training_labels) # training data lables(answers)\n",
    "        if self.tx.shape[0] != self.ty.shape[0]:\n",
    "            temp_min = min(self.tx.shape[0], self.ty.shape[0])\n",
    "            self.tx = self.tx[:temp_min]\n",
    "            self.ty = self.ty[:temp_min]\n",
    "            print(\"training data #input != #output, took the minimun size automatically\")\n",
    "        \n",
    "        self.xs = self.tx.shape[1] # size of each datum input\n",
    "        self.ys = self.ty.shape[1] # size of each datum output\n",
    "    \n",
    "    def set_validation_data(self, validation_input, validation_labels):\n",
    "        self.vx = np.array(validation_input) # validation data input\n",
    "        self.vy = np.array(validation_labels) # validation data lables(answers)\n",
    "        if self.vx.shape[1] != self.xs:\n",
    "            raise ValueError(\"validation data input size should be equal to training data\")\n",
    "        \n",
    "        if self.vy.shape[1] != self.ys:\n",
    "            raise ValueError(\"validation data lables size should be equal to training data\")\n",
    "    \n",
    "    def add_layer(self, new_layer):\n",
    "        if type(new_layer) != Layer:\n",
    "            raise TypeError(\"new_layer should be a Layer (class). eg: 'Layer(30, Sigmoid())'\")\n",
    "        \n",
    "        self.ly.append(new_layer)\n",
    "    \n",
    "    def build(self):\n",
    "        self.ln = len(self.ly) # amount of layers\n",
    "        self.ly[0].w.assign_values(np.random.normal(0., 1., (self.ly[0].nn, self.xs)), self.csi)\n",
    "        self.ly[0].b.assign_values(np.random.normal(0., 1., (self.ly[0].nn, 1)), self.csi)\n",
    "        for l in range(1,self.ln):\n",
    "            self.ly[l].w.assign_values(np.random.normal(0., 1., (self.ly[l].nn, self.ly[l-1].nn)), self.csi)\n",
    "            self.ly[l].b.assign_values(np.random.normal(0., 1., (self.ly[l].nn, 1)), self.csi)\n",
    "        \n",
    "        if self.ly[-1].nn != self.ys: # cheak output size\n",
    "            raise ValueError(\"output layer must has the same size with datum lables(answer)\")\n",
    "    \n",
    "    def prediction(self, data_input):\n",
    "        self.px = np.array(data_input) # prediction data input of last time predic\n",
    "        if self.px.shape[1] != self.xs:\n",
    "            raise ValueError(\"datum size error\")\n",
    "        \n",
    "        self.ly[0].forward(self.px.T)\n",
    "        for l in range(1,self.ln):\n",
    "            self.ly[l].forward(self.ly[l-1].y)\n",
    "        \n",
    "        self.py = np.array(self.ly[-1].y.T) # prediction result of last time predict\n",
    "        \n",
    "        return self.py\n",
    "    \n",
    "    def descent(self, step = 1., descent_method = \"normal\"):\n",
    "        for l in range(self.ln):\n",
    "            self.ly[l].descent(step, descent_method, self.rg)\n",
    "        \n",
    "        if descent_method in [\"Rprop\", \"Dogiko Rprop\"]:\n",
    "            self.max_cs = 0.\n",
    "            for l in range(self.ln):\n",
    "                self.max_cs = max(self.max_cs, self.ly[l].w.max_cs(), self.ly[l].b.max_cs())\n",
    "    \n",
    "    def evaluate(self, _input, labels):\n",
    "        self.prediction(_input)\n",
    "        if self.lf == \"r2\":\n",
    "            return np.square(self.py - labels).mean()/labels.var(axis=0).mean()\n",
    "        elif self.lf == \"ce\":\n",
    "            return (-1*labels*np.log(self.py+0.0001)).sum(axis=1).mean()\n",
    "        else:\n",
    "            raise ValueError(\"loss function should be 'r2' or 'ce'\")\n",
    "    \n",
    "    def gradient_get(self, _input, labels):\n",
    "        self.prediction(_input)\n",
    "        if self.lf == \"r2\":\n",
    "            temp_derivative = 2*(self.py - labels).T/(labels.shape[0]*labels.var(axis=0).sum())\n",
    "        elif self.lf == \"ce\":\n",
    "            temp_derivative = -1*(labels/(self.py + 0.0001)).T/labels.shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"loss function should be 'r2' or 'ce'\")\n",
    "        \n",
    "        for l in range(self.ln-1, 0, -1):\n",
    "            temp_derivative = self.ly[l].backward(temp_derivative, self.ly[l-1].y)\n",
    "        \n",
    "        self.ly[0].backward(temp_derivative, _input.T)\n",
    "    \n",
    "    def batch_fit(self, batch_input, batch_labels, step = 1., descent_method = \"normal\"):\n",
    "        self.gradient_get(batch_input, batch_labels)\n",
    "        self.descent(step, descent_method)\n",
    "    \n",
    "    def epoch_fit(self, batch_size = None, step = 1., descent_method = \"normal\"):\n",
    "        if type(batch_size) == type(None):\n",
    "            self.batch_fit(self.tx, self.ty, step, descent_method)\n",
    "        elif type(batch_size) == int:\n",
    "            if batch_size > 0:\n",
    "                for b in range(np.ceil(self.tx.shape[0]/ batch_size).astype(np.int)):\n",
    "                    self.batch_fit(self.tx[b*batch_size: (b+1)*batch_size],\n",
    "                                   self.ty[b*batch_size: (b+1)*batch_size],\n",
    "                                   step,\n",
    "                                   descent_method\n",
    "                                  )\n",
    "            else:\n",
    "                raise ValueError(\"batch_size should be positive int\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"batch_size should be positive int\")\n",
    "    \n",
    "    def train(self, times, batch_size = None, step = 1., descent_method = \"normal\", termination = [0,0,0.]):\n",
    "        is_termination = False\n",
    "        try:\n",
    "            termination[2] > 12345 # test whether threshold is an real number (no mater int, float,.etc)\n",
    "            termination[0] = int(termination[0])\n",
    "            termination[1] = int(termination[1])\n",
    "            if (termination[0] < termination[1]) and (termination[0] > 0):\n",
    "                is_termination = True\n",
    "                error_record = []\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        for t in range(times):\n",
    "            self.epoch_fit(batch_size, step, descent_method)\n",
    "            if is_termination:\n",
    "                if self.lf == \"ce\":\n",
    "                    error_record.append(10*np.log10(1.000000001 - self.validation_accuracy()))\n",
    "                else:\n",
    "                    error_record.append(10*np.log10(self.validation_error()+0.000000001))\n",
    "                # 0.000000001, bias for prevent error when log(0)\n",
    "                \n",
    "                if t >= (termination[1] - 1):\n",
    "                    short_mean = sum(error_record[(-termination[0]):])/termination[0]\n",
    "                    long_mean = sum(error_record[(-termination[1]):])/termination[1]\n",
    "                    if (long_mean - short_mean) < termination[2]:\n",
    "                        return t+1\n",
    "        \n",
    "        return times\n",
    "            \n",
    "    def accuracy(self, inference, target):\n",
    "        if inference.shape != target.shape:\n",
    "            raise ValueError(\"shape of inference and target non-equal\")\n",
    "            \n",
    "        return (inference.argmax(axis=1) == target.argmax(axis=1)).sum()/inference.shape[0]\n",
    "    \n",
    "    def training_error(self):\n",
    "        return self.evaluate(self.tx, self.ty)\n",
    "    \n",
    "    def training_accuracy(self):\n",
    "        return self.accuracy(self.prediction(self.tx), self.ty)\n",
    "    \n",
    "    def validation_error(self):\n",
    "        return self.evaluate(self.vx, self.vy)\n",
    "    \n",
    "    def validation_accuracy(self):\n",
    "        return self.accuracy(self.prediction(self.vx), self.vy)\n",
    "    \n",
    "    def neuron_refined(self, l, reference_data = None, threshold = 0.01):\n",
    "        # l : the # of layer\n",
    "        # threshold : threshold for information contained of dimension be remaind\n",
    "        if type(l) != int:\n",
    "            raise TypeError(\"l should be the layer no. of hidden layer, an int between 0 to (neural_number - 2)\")\n",
    "        elif (l >= self.ln - 1) or (l < 0):\n",
    "            raise ValueError(\"l should be the layer no. of hidden layer, an int between 0 to (neural_number - 2)\")\n",
    "        \n",
    "        try:\n",
    "            if ((threshold< 1) and (threshold>0)) or (type(threshold) == int):\n",
    "                if (threshold > self.ly[l].nn-1):\n",
    "                    raise ValueError(\"int threshold error : removed #neuron should less than currently #neuron\")\n",
    "                elif (threshold < -self.ly[l].nn) or (threshold==0):\n",
    "                    return None\n",
    "                    # do nothing if remove no #neuron (threshold=0) or want to remain #neuron more than currently\n",
    "            else:\n",
    "                raise ValueError(\"threshold : a value in (0, 1), or an nonzero int\")\n",
    "        except:\n",
    "            raise ValueError(\"threshold : a value in (0, 1), or an nonzero int\")\n",
    "        \n",
    "        if type(reference_data) == type(None):\n",
    "            self.prediction(self.tx)\n",
    "        else:\n",
    "            self.prediction(reference_data)\n",
    "        \n",
    "        ym = self.ly[l].y.mean(axis=1).reshape((self.ly[l].nn,1)) # y (output of Layer) mean of each neurons\n",
    "        yn = self.ly[l].y - ym # centralized y\n",
    "        ab = np.dot(self.ly[l+1].w.v, ym) # Adjusted bias\n",
    "        ev, em = np.linalg.eigh(np.dot(yn, yn.T)) # eigenvalues and eigenmatrix(with eigenvectors as columns)\n",
    "        ir = ev/ev.sum() # info ratio for each eigenvector\n",
    "        # op, pv :column operator result and pivots\n",
    "        if (threshold< 1) and (threshold>0):\n",
    "            op, pv = column_operate(em[:,ir > threshold])\n",
    "        else:\n",
    "            op, pv = column_operate(em[:,ir >= ir[ir.argsort()[threshold]]])\n",
    "            \n",
    "        nw = np.dot(self.ly[l+1].w.v, op) # new weight\n",
    "        self.ly[l+1].b.assign_values(self.ly[l+1].b.v + (np.dot(self.ly[l+1].w.v, ym) -np.dot(nw, ym[pv])))\n",
    "        self.ly[l+1].w.assign_values(nw) # l+1 weight should be rewrite after l+1 bias have been rewrite\n",
    "        self.ly[l].w.assign_values(self.ly[l].w.v[pv])\n",
    "        self.ly[l].b.assign_values(self.ly[l].b.v[pv])\n",
    "        self.ly[l].nn = len(pv)\n",
    "    \n",
    "    def neuron_proliferate(self, proliferating_layer, proliferating_n = 1, output_weight_bound = 1.):\n",
    "        if proliferating_layer not in range(self.ln):\n",
    "            raise ValueError(\"proliferating_layer should be an int from 0 to (#layer-1)\")\n",
    "            \n",
    "        if type(proliferating_n) != int:\n",
    "            raise ValueError(\"proliferating_n should be int\")\n",
    "        \n",
    "        if proliferating_n <= 0:\n",
    "            raise ValueError(\"proliferating_n should be postive\")\n",
    "            \n",
    "        if output_weight_bound < 0.:\n",
    "            raise ValueError(\"output_weight_bound should be non-negative\")\n",
    "            \n",
    "        l = proliferating_layer\n",
    "        for t in range(proliferating_n):\n",
    "            self.ly[l].w.add_row(mcmc_normal(self.ly[l].w.v, mean=self.ly[l].w.v.mean(), std=self.ly[l].w.v.std()))\n",
    "            self.ly[l].b.add_row(mcmc_normal(self.ly[l].b.v, mean=self.ly[l].b.v.mean(), std=self.ly[l].b.v.std()))\n",
    "            self.ly[l+1].w.add_column(output_weight_bound*(2*np.random.rand((self.ly[l+1].nn))-1.))\n",
    "            self.ly[l].nn += 1\n",
    "    \n",
    "    def reset_cs(self, new_cs):\n",
    "        for l in range(self.ln):\n",
    "            self.ly[l].reset_cs(new_cs)\n",
    "    \n",
    "    def inter_layer_linear_regression(self, layer_interval):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                if ls == 0:\n",
    "                    ri = np.array(self.px.T) # regression input\n",
    "                else:\n",
    "                    ri = np.array(self.ly[ls-1].y)\n",
    "                \n",
    "                ri = np.append(ri, np.ones((1, ri.shape[1])), axis=0) # append 1. for each datum as bias\n",
    "                ro = np.array(self.ly[le].x)\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        rr = np.linalg.lstsq(ri.T, ro.T) # regression result (matrix, residuals, rank of ri, singuler values of ri)\n",
    "        if len(rr[1]) == 0:\n",
    "            raise ValueError(\"output data of layer\" + str(ls-1) + \"(= -1, for input data) should be full rank, try self.nruron_refine first\")\n",
    "        \n",
    "        return rr[0], rr[1]/ri.shape[1]\n",
    "    \n",
    "    def find_linearist_layers(self, reference_data = None):\n",
    "        output = (0, 0, np.inf, np.array([[]]), np.zeros((0,0)))\n",
    "        if type(reference_data) == type(None):\n",
    "            self.prediction(self.tx)\n",
    "        else:\n",
    "            self.prediction(reference_data)\n",
    "        \n",
    "        for l1 in range(self.ln-1):\n",
    "            for l2 in range(i+1, self.ln):\n",
    "                rr = self.inter_layer_linear_regression((l1,l2))\n",
    "                if np.sqrt(rr[1].sum()) < output[2]:\n",
    "                    output = (l1, l2, np.sqrt(rr[1].sum()), rr[0])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def layer_filled(self, layer_interval, weights, bias):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        if weights.shape[0] != bias.shape[0]:\n",
    "            raise ValueError(\"weights.shape[0] doesn't match bias.shape[0]\")\n",
    "        \n",
    "        if weights.shape[0] != self.ly[le].nn:\n",
    "            raise ValueError(\"weights.shape[0] doesn't match #neuron of layer at end of layer_interval\")\n",
    "        \n",
    "        self.ly[le].w.assign_values(weights)\n",
    "        self.ly[le].b.assign_values(bias)\n",
    "        self.ly = self.ly[:ls] + self.ly[le:]\n",
    "        self.ln = len(self.ly)\n",
    "    \n",
    "    def linear_filled(self, layer_interval):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "            \n",
    "        rr = self.inter_layer_linear_regression(layer_interval)\n",
    "        self.layer_filled(layer_interval, rr[0].T[:,:-1], rr[0].T[:,-1:])\n",
    "    \n",
    "    def insert_layer(self, position, weights, bias, activation_function, next_layer_weights, next_layer_bias):\n",
    "        if type(position) == int:\n",
    "            if position in range(self.ln):\n",
    "                pass\n",
    "        else:\n",
    "            raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        \n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        ilo, ili = weights.shape # input and output size of inserted layer\n",
    "        nlo, nli = next_layer_weights.shape # input and output size of next layer\n",
    "        \n",
    "        if position == 0:\n",
    "            if ili != self.xs:\n",
    "                raise ValueError(\"weights.shape error, cheak input and output size for this new layer\")\n",
    "        else:\n",
    "            if ili != self.ly[position-1].nn:\n",
    "                raise ValueError(\"weights.shape error, cheak input and output size for this new layer\")\n",
    "        \n",
    "        if (ilo != bias.shape[0]) or (ilo != nli):\n",
    "            raise ValueError(\"to define #neuron of new layer, all related weighs and bias size should be consistent\")\n",
    "        \n",
    "        if nlo != self.ly[position].nn:\n",
    "            raise ValueError(\"next_layer_weights.shape error, cheak #neuron of next layer\")\n",
    "        \n",
    "        if next_layer_bias.shape[0] != self.ly[position].nn:\n",
    "            raise ValueError(\"next_layer_bias.shape error, cheak #neuron of next layer\")\n",
    "        \n",
    "        if (bias.shape[1] != 1) or (next_layer_bias.shape[1] != 1):\n",
    "            raise ValueError(\"bias shape should be (#neuron, 1)\")\n",
    "        \n",
    "        l = position\n",
    "        \n",
    "        self.ly.insert(l, Layer(ilo, activation_function))\n",
    "        self.ly[l].w.assign_values(weights)\n",
    "        self.ly[l].b.assign_values(bias)\n",
    "        self.ly[l+1].w.assign_values(next_layer_weights)\n",
    "        self.ly[l+1].b.assign_values(next_layer_bias)\n",
    "        \n",
    "        self.ln = len(self.ly)\n",
    "    \n",
    "    def identity_dig(self, position, activation_function):\n",
    "        if type(position) == int:\n",
    "            if position in range(self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        else:\n",
    "            raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        \n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        l = position\n",
    "        # ids : size of identity transform, input size of new layer\n",
    "        if l == 0:\n",
    "            ids = self.xs\n",
    "        else:\n",
    "            ids = self.ly[l-1].nn\n",
    "        \n",
    "        if type(activation_function) in [Relu, SoftPlus]:\n",
    "            liw = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 0)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 1)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) == LeakyRelu:\n",
    "            liw = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 0)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 1) / (1.+activation_function.alpha)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) == Identity:\n",
    "            liw = np.identity(ids)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.identity(ids)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) in [Sigmoid, Hypertan, Selu]:\n",
    "            # li : input of new layer\n",
    "            if l == 0:\n",
    "                li = np.array(self.tx.T)\n",
    "            else:\n",
    "                li = np.array(self.ly[l-1].y)\n",
    "            \n",
    "            lim = li.mean(axis=1)\n",
    "            lis = li.std(axis=1) + 1.\n",
    "            \n",
    "            liw = np.diag(1./lis)\n",
    "            if type(activation_function) == Selu:\n",
    "                lib = 1.-(lim/lis).reshape(-1,1) # let mean become one before transform by activation function\n",
    "            else:\n",
    "                lib = -(lim/lis).reshape(-1,1) # let mean become zero before transform by activation function\n",
    "            \n",
    "            lo = activation_function.trans(np.dot(liw, li)+lib)\n",
    "            lo = np.append(lo, np.ones((1, lo.shape[1])), axis=0) # append 1. for each datum as bias\n",
    "            rr = np.linalg.lstsq(lo.T, li.T) # regression result (matrix, residuals, rank of ri, singuler values of ri)\n",
    "            # since the goal is construct identity, try to find linear transform form layer output to layer input\n",
    "            low = rr[0].T[:,:-1]\n",
    "            lob = rr[0].T[:,-1:]\n",
    "        else:\n",
    "            raise TypeError(\"activation_function type error\")\n",
    "        \n",
    "        nlw = np.dot(self.ly[l].w.v, low)\n",
    "        nlb = np.dot(self.ly[l].w.v, lob) + self.ly[l].b.v\n",
    "        \n",
    "        self.insert_layer(l,\n",
    "                          liw,\n",
    "                          lib,\n",
    "                          activation_function,\n",
    "                          nlw,\n",
    "                          nlb\n",
    "                         )\n",
    "    \n",
    "    def dimension(self):\n",
    "        output = 0\n",
    "        for l in range(self.ln):\n",
    "            output += self.ly[l].dimension()\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def save_weight(self, dir_name):\n",
    "        for l in range(self.ln):\n",
    "            np.save(dir_name + \"/w%i.npy\" % l, self.ly[l].w.v)\n",
    "            np.save(dir_name + \"/b%i.npy\" % l, self.ly[l].b.v)\n",
    "    \n",
    "    def load_weight(self, dir_name):\n",
    "        for l in range(self.ln):\n",
    "            try:\n",
    "                if l == 0:\n",
    "                    if np.load(dir_name + \"/w%i.npy\" % l).shape[1] != self.xs:\n",
    "                        raise ValueError(\"layer %i input size error, cheak weight size.\" % l)\n",
    "                else:\n",
    "                    if np.load(dir_name + \"/w%i.npy\" % l).shape[1] != self.ly[l-1].nn:\n",
    "                        raise ValueError(\"layer %i input size error, cheak weight size.\" % l)\n",
    "\n",
    "                if np.load(dir_name + \"/w%i.npy\" % l).shape[0] != self.ly[l].nn:\n",
    "                    raise ValueError(\"layer %i neuron size error, cheak weight size.\" % l)\n",
    "\n",
    "                if np.load(dir_name + \"/b%i.npy\" % l).shape[0] != self.ly[l].nn:\n",
    "                    raise ValueError(\"layer %i neuron size error, cheak bias size.\" % l)\n",
    "\n",
    "                if np.load(dir_name + \"/b%i.npy\" % l).shape[1] != 1:\n",
    "                    raise ValueError(\"layer %i bias size error, should be 1.\" % l)\n",
    "            \n",
    "            except:\n",
    "                raise ValueError(\"load .npy error, cheak dir.\")\n",
    "            \n",
    "            self.ly[l].w.assign_values(np.load(dir_name + \"/w%i.npy\" % l))\n",
    "            self.ly[l].b.assign_values(np.load(dir_name + \"/b%i.npy\" % l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red wine classification\n",
    "\n",
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine quality from 3 to 8, totally 6 classes\n"
     ]
    }
   ],
   "source": [
    "red_data = np.load(\"data-npy/red_data.npy\")\n",
    "red_label = np.load(\"data-npy/red_label.npy\")\n",
    "\n",
    "# data normalization\n",
    "red_data -= red_data.mean(axis=0)\n",
    "red_data /= red_data.std(axis=0)\n",
    "\n",
    "# label one-head\n",
    "red_label = red_label.astype(np.int)\n",
    "red_label_one_head = np.zeros((red_label.shape[0], red_label.max()-red_label.min() + 1))\n",
    "print (\"wine quality from %d to %d, totally %d classes\"\n",
    "       %(red_label.min(), red_label.max(), red_label.max() - red_label.min() + 1))\n",
    "\n",
    "for j in range(red_label.max() - red_label.min() + 1):\n",
    "    red_label_one_head[:,j] = (red_label.reshape(-1)==(j + red_label.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle = np.arange(red_data.shape[0])\n",
    "np.random.shuffle(shuffle)\n",
    "\n",
    "remain_data = red_data[shuffle[:1400]]\n",
    "remain_label = red_label_one_head[shuffle[:1400]]\n",
    "\n",
    "test_data = red_data[shuffle[1400:]]\n",
    "test_label = red_label_one_head[shuffle[1400:]]\n",
    "\n",
    "train_data = remain_data[:1200]\n",
    "train_label = remain_label[:1200]\n",
    "\n",
    "valid_data = remain_data[1200:]\n",
    "valid_label = remain_label[1200:]\n",
    "\n",
    "redWineNN = DogikoLearn(loss_function=\"ce\")\n",
    "redWineNN.rs_extend_regularizer(0.001, 3.)\n",
    "redWineNN.set_training_data(train_data, train_label)\n",
    "redWineNN.set_validation_data(valid_data, valid_label)\n",
    "redWineNN.add_layer(Layer(20, Hypertan()))\n",
    "redWineNN.add_layer(Layer(20, Hypertan()))\n",
    "redWineNN.add_layer(Layer(6, Softmax()))\n",
    "redWineNN.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8, 9, 0.6875 0.605\n",
      "8, 8, 0.698333333333 0.6\n",
      "9, 8, 0.716666666667 0.575\n",
      "9, 8, 0.718333333333 0.575\n",
      "10, 8, 0.720833333333 0.61\n",
      "10, 8, 0.733333333333 0.62\n",
      "11, 7, 0.703333333333 0.595\n",
      "12, 8, 0.753333333333 0.57\n",
      "11, 8, 0.744166666667 0.58\n",
      "12, 8, 0.760833333333 0.575\n",
      "11, 8, 0.749166666667 0.61\n",
      "11, 8, 0.749166666667 0.595\n",
      "12, 8, 0.751666666667 0.57\n",
      "12, 8, 0.7625 0.55\n",
      "11, 8, 0.743333333333 0.58\n",
      "11, 8, 0.756666666667 0.56\n",
      "11, 8, 0.7625 0.555\n",
      "11, 9, 0.776666666667 0.57\n",
      "11, 9, 0.785833333333 0.585\n",
      "11, 10, 0.796666666667 0.575\n",
      "11, 9, 0.7475 0.54\n",
      "11, 9, 0.774166666667 0.515\n",
      "11, 9, 0.735833333333 0.5\n",
      "11, 9, 0.7525 0.515\n",
      "11, 10, 0.761666666667 0.5\n",
      "11, 10, 0.765 0.515\n",
      "11, 10, 0.7775 0.495\n",
      "11, 10, 0.780833333333 0.495\n",
      "11, 10, 0.738333333333 0.505\n",
      "11, 10, 0.765 0.515\n",
      "11, 9, 0.750833333333 0.525\n",
      "11, 9, 0.745833333333 0.54\n",
      "11, 9, 0.760833333333 0.54\n",
      "11, 9, 0.7575 0.53\n",
      "12, 8, 0.779166666667 0.57\n",
      "12, 9, 0.791666666667 0.565\n",
      "11, 9, 0.773333333333 0.535\n",
      "11, 9, 0.758333333333 0.565\n",
      "12, 8, 0.770833333333 0.59\n",
      "12, 8, 0.796666666667 0.59\n",
      "12, 8, 0.7975 0.58\n",
      "12, 9, 0.8125 0.605\n",
      "12, 9, 0.815 0.605\n",
      "12, 9, 0.815 0.595\n",
      "12, 9, 0.766666666667 0.575\n",
      "11, 10, 0.741666666667 0.545\n",
      "11, 9, 0.7275 0.56\n",
      "12, 9, 0.7275 0.59\n",
      "12, 8, 0.7225 0.615\n",
      "12, 8, 0.739166666667 0.625\n",
      "11, 9, 0.7525 0.575\n",
      "11, 9, 0.723333333333 0.605\n",
      "11, 9, 0.775833333333 0.61\n",
      "11, 9, 0.7275 0.585\n",
      "11, 9, 0.7525 0.545\n",
      "11, 9, 0.721666666667 0.535\n",
      "11, 9, 0.7425 0.55\n",
      "11, 10, 0.751666666667 0.56\n",
      "11, 10, 0.763333333333 0.565\n",
      "11, 10, 0.7675 0.57\n",
      "11, 10, 0.775 0.6\n",
      "12, 10, 0.780833333333 0.575\n",
      "11, 10, 0.746666666667 0.59\n",
      "11, 10, 0.751666666667 0.585\n",
      "11, 10, 0.749166666667 0.57\n",
      "11, 11, 0.7675 0.59\n",
      "11, 11, 0.779166666667 0.58\n",
      "11, 11, 0.776666666667 0.59\n",
      "11, 11, 0.785 0.585\n",
      "11, 11, 0.781666666667 0.59\n",
      "11, 11, 0.741666666667 0.6\n",
      "11, 10, 0.7325 0.59\n",
      "11, 11, 0.754166666667 0.575\n",
      "10, 10, 0.733333333333 0.55\n",
      "10, 10, 0.73 0.58\n",
      "10, 10, 0.716666666667 0.605\n",
      "10, 10, 0.750833333333 0.585\n",
      "10, 9, 0.765833333333 0.585\n",
      "10, 9, 0.784166666667 0.585\n",
      "10, 9, 0.744166666667 0.55\n",
      "11, 9, 0.7475 0.565\n",
      "11, 9, 0.753333333333 0.535\n",
      "11, 9, 0.7475 0.545\n",
      "11, 9, 0.763333333333 0.525\n",
      "11, 9, 0.756666666667 0.56\n",
      "11, 9, 0.759166666667 0.55\n",
      "11, 9, 0.769166666667 0.55\n",
      "11, 9, 0.740833333333 0.535\n",
      "12, 10, 0.775 0.545\n",
      "12, 10, 0.755833333333 0.525\n",
      "12, 9, 0.768333333333 0.555\n",
      "12, 9, 0.76 0.565\n",
      "12, 9, 0.763333333333 0.52\n",
      "11, 9, 0.7425 0.56\n",
      "11, 9, 0.760833333333 0.56\n",
      "11, 10, 0.769166666667 0.57\n",
      "11, 9, 0.7425 0.54\n",
      "11, 10, 0.78 0.56\n",
      "11, 10, 0.743333333333 0.575\n",
      "11, 10, 0.746666666667 0.56\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for t in range(100):\n",
    "    shuffle = np.arange(remain_data.shape[0])\n",
    "    np.random.shuffle(shuffle)\n",
    "    \n",
    "    remain_data = remain_data[shuffle]\n",
    "    remain_label = remain_label[shuffle]\n",
    "\n",
    "    train_data = remain_data[:1200]\n",
    "    train_label = remain_label[:1200]\n",
    "\n",
    "    valid_data = remain_data[1200:]\n",
    "    valid_label = remain_label[1200:]\n",
    "    for l in range(redWineNN.ln-1):\n",
    "        redWineNN.neuron_proliferate(proliferating_layer=l,\n",
    "                                     proliferating_n=1,\n",
    "                                     output_weight_bound=0.001/np.sqrt(redWineNN.ly[l+1].nn)\n",
    "                                    )\n",
    "        redWineNN.train(1000, descent_method=\"Rprop\", termination=[10,30,0.])\n",
    "        redWineNN.neuron_refined(l,\n",
    "                                 reference_data=remain_data,\n",
    "                                 threshold=redWineNN.dimension()/(1200*redWineNN.ly[l].nn)\n",
    "                                )\n",
    "        redWineNN.train(1000, descent_method=\"Rprop\", termination=[10,30,0.])\n",
    "        print(redWineNN.ly[l].nn, end=\", \")\n",
    "    \n",
    "    print(redWineNN.training_accuracy(),\n",
    "          redWineNN.validation_accuracy()\n",
    "         )\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance cheak\n",
    "\n",
    "Cheaking covariance-matrix of softmax output values.\n",
    "\n",
    "Positive for covariance near diagonal shows that continuous value batter than one-head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True, False, False, False],\n",
       "       [False, False, False,  True, False, False],\n",
       "       [False, False, False, False,  True,  True],\n",
       "       [False, False, False, False,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redWineNN.training_accuracy\n",
    "out = redWineNN.ly[-1].y\n",
    "out -= out.mean(axis=1).reshape(-1,1)\n",
    "out /= out.std(axis=1).reshape(-1,1)\n",
    "\n",
    "np.dot(out, out.T)>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# White wine classification\n",
    "\n",
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine quality from 3 to 9, totally 7 classes\n"
     ]
    }
   ],
   "source": [
    "white_data = np.load(\"data-npy/white_data.npy\")\n",
    "white_label = np.load(\"data-npy/white_label.npy\")\n",
    "\n",
    "# data normalization\n",
    "white_data -= white_data.mean(axis=0)\n",
    "white_data /= white_data.std(axis=0)\n",
    "\n",
    "# label one-head\n",
    "white_label = white_label.astype(np.int)\n",
    "white_label_one_head = np.zeros((white_label.shape[0], white_label.max() - white_label.min() + 1))\n",
    "print (\"wine quality from %d to %d, totally %d classes\"\n",
    "       %(white_label.min(), white_label.max(), white_label.max() - white_label.min() + 1))\n",
    "\n",
    "for j in range(white_label.max() - white_label.min() + 1):\n",
    "    white_label_one_head[:,j] = (white_label.reshape(-1)==(j + white_label.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle = np.arange(white_data.shape[0])\n",
    "np.random.shuffle(shuffle)\n",
    "\n",
    "remain_data = white_data[shuffle[:4400]]\n",
    "remain_label = white_label_one_head[shuffle[:4400]]\n",
    "\n",
    "test_data = white_data[shuffle[4400:]]\n",
    "test_label = white_label_one_head[shuffle[4400:]]\n",
    "\n",
    "train_data = remain_data[:4000]\n",
    "train_label = remain_label[:4000]\n",
    "\n",
    "valid_data = remain_data[4000:]\n",
    "valid_label = remain_label[4000:]\n",
    "\n",
    "whiteWineNN = DogikoLearn(loss_function=\"ce\")\n",
    "whiteWineNN.rs_extend_regularizer(0.001, 3.)\n",
    "whiteWineNN.set_training_data(train_data, train_label)\n",
    "whiteWineNN.set_validation_data(valid_data, valid_label)\n",
    "whiteWineNN.add_layer(Layer(20, Hypertan()))\n",
    "whiteWineNN.add_layer(Layer(20, Hypertan()))\n",
    "whiteWineNN.add_layer(Layer(7, Softmax()))\n",
    "whiteWineNN.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18, 26, 0.7135 0.53\n",
      "18, 27, 0.726 0.5625\n",
      "19, 26, 0.71325 0.5075\n",
      "19, 26, 0.71825 0.555\n",
      "19, 25, 0.67475 0.5425\n",
      "19, 25, 0.7005 0.5425\n",
      "19, 25, 0.6915 0.55\n",
      "19, 25, 0.71675 0.5425\n",
      "19, 25, 0.71975 0.5425\n",
      "19, 26, 0.73275 0.5475\n",
      "19, 26, 0.72825 0.53\n",
      "19, 25, 0.712 0.5325\n",
      "19, 25, 0.688 0.4925\n",
      "18, 25, 0.69825 0.5225\n",
      "18, 25, 0.71325 0.545\n",
      "18, 25, 0.7085 0.5375\n",
      "18, 26, 0.7075 0.57\n",
      "18, 25, 0.707 0.555\n",
      "18, 26, 0.71925 0.5475\n",
      "19, 25, 0.716 0.5425\n",
      "18, 25, 0.7035 0.535\n",
      "19, 25, 0.71825 0.545\n",
      "19, 25, 0.7135 0.525\n",
      "18, 25, 0.675 0.5375\n",
      "18, 25, 0.68725 0.5575\n",
      "18, 24, 0.69625 0.555\n",
      "19, 24, 0.7095 0.55\n",
      "19, 25, 0.71025 0.535\n",
      "19, 25, 0.71075 0.5425\n",
      "19, 25, 0.6845 0.5025\n",
      "18, 25, 0.695 0.5225\n",
      "18, 26, 0.71025 0.515\n",
      "19, 25, 0.70325 0.5325\n",
      "20, 24, 0.722 0.53\n",
      "20, 23, 0.71875 0.535\n",
      "21, 23, 0.70675 0.53\n",
      "21, 23, 0.7245 0.54\n",
      "20, 24, 0.7085 0.595\n",
      "20, 25, 0.71825 0.5675\n",
      "19, 25, 0.70925 0.56\n",
      "19, 25, 0.7335 0.5875\n",
      "19, 25, 0.7335 0.585\n",
      "19, 25, 0.72025 0.5825\n",
      "19, 25, 0.73225 0.58\n",
      "19, 25, 0.70525 0.58\n",
      "19, 25, 0.7185 0.5675\n",
      "19, 26, 0.70425 0.535\n",
      "18, 26, 0.69275 0.5625\n",
      "19, 25, 0.69875 0.555\n",
      "19, 25, 0.7165 0.5525\n",
      "19, 25, 0.72375 0.5425\n",
      "19, 26, 0.7345 0.5525\n",
      "19, 26, 0.73325 0.5575\n",
      "19, 27, 0.73075 0.5625\n",
      "19, 27, 0.735 0.5525\n",
      "19, 27, 0.73325 0.58\n",
      "19, 26, 0.73775 0.5625\n",
      "19, 27, 0.74 0.55\n",
      "19, 27, 0.74625 0.5525\n",
      "19, 27, 0.7575 0.5525\n",
      "19, 27, 0.75625 0.55\n",
      "19, 27, 0.70425 0.5125\n",
      "19, 27, 0.72475 0.51\n",
      "19, 27, 0.72775 0.5375\n",
      "18, 28, 0.72675 0.5225\n",
      "18, 28, 0.73225 0.52\n",
      "18, 28, 0.7095 0.51\n",
      "18, 27, 0.69375 0.5025\n",
      "18, 28, 0.71425 0.5275\n",
      "18, 28, 0.689 0.4825\n",
      "18, 29, 0.716 0.525\n",
      "18, 29, 0.7315 0.5525\n",
      "18, 29, 0.73825 0.55\n",
      "18, 28, 0.72325 0.5025\n",
      "19, 28, 0.731 0.5175\n",
      "19, 27, 0.71775 0.5475\n",
      "19, 27, 0.7335 0.54\n",
      "19, 27, 0.723 0.525\n",
      "19, 27, 0.7325 0.53\n",
      "19, 27, 0.73875 0.5225\n",
      "19, 27, 0.74375 0.525\n",
      "19, 28, 0.7565 0.5325\n",
      "19, 28, 0.75375 0.54\n",
      "18, 28, 0.7175 0.5175\n",
      "19, 28, 0.73775 0.5175\n",
      "19, 28, 0.733 0.5125\n",
      "19, 28, 0.7205 0.53\n",
      "19, 28, 0.73475 0.5375\n",
      "19, 29, 0.74775 0.5175\n",
      "18, 28, 0.72125 0.5425\n",
      "18, 27, 0.7165 0.535\n",
      "18, 28, 0.719 0.52\n",
      "18, 28, 0.7305 0.5375\n",
      "19, 28, 0.741 0.555\n",
      "18, 28, 0.73725 0.545\n",
      "18, 29, 0.73875 0.5375\n",
      "19, 28, 0.7295 0.535\n",
      "19, 28, 0.74175 0.54\n",
      "19, 28, 0.72975 0.5325\n",
      "19, 28, 0.74625 0.5425\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for t in range(100):\n",
    "    shuffle = np.arange(remain_data.shape[0])\n",
    "    np.random.shuffle(shuffle)\n",
    "    \n",
    "    remain_data = remain_data[shuffle]\n",
    "    remain_label = remain_label[shuffle]\n",
    "\n",
    "    train_data = remain_data[:4000]\n",
    "    train_label = remain_label[:4000]\n",
    "\n",
    "    valid_data = remain_data[4000:]\n",
    "    valid_label = remain_label[4000:]\n",
    "    for l in range(whiteWineNN.ln-1):\n",
    "        whiteWineNN.neuron_proliferate(proliferating_layer=l,\n",
    "                                     proliferating_n=1,\n",
    "                                     output_weight_bound=0.01/np.sqrt(whiteWineNN.ly[l+1].nn)\n",
    "                                    )\n",
    "        whiteWineNN.train(1000, descent_method=\"Rprop\", termination=[10,30,0.])\n",
    "        whiteWineNN.neuron_refined(l,\n",
    "                                 reference_data=remain_data,\n",
    "                                 threshold=whiteWineNN.dimension()/(4400*whiteWineNN.ly[l].nn)\n",
    "                                )\n",
    "        whiteWineNN.train(1000, descent_method=\"Rprop\", termination=[10,30,0.])\n",
    "        print(whiteWineNN.ly[l].nn, end=\", \")\n",
    "    \n",
    "    print(whiteWineNN.training_accuracy(),\n",
    "          whiteWineNN.validation_accuracy()\n",
    "         )\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance cheak\n",
    "\n",
    "Cheaking covariance-matrix of softmax output values.\n",
    "\n",
    "Positive for covariance near diagonal shows that continuous value batter than one-head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False, False, False],\n",
       "       [False,  True,  True, False, False, False, False],\n",
       "       [False,  True,  True, False, False, False, False],\n",
       "       [False, False, False,  True, False, False, False],\n",
       "       [False, False, False, False,  True,  True,  True],\n",
       "       [False, False, False, False,  True,  True,  True],\n",
       "       [False, False, False, False,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whiteWineNN.training_accuracy\n",
    "out = whiteWineNN.ly[-1].y\n",
    "out -= out.mean(axis=1).reshape(-1,1)\n",
    "out /= out.std(axis=1).reshape(-1,1)\n",
    "\n",
    "np.dot(out, out.T)>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Red wine regression\n",
    "\n",
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "red_data = np.load(\"data-npy/red_data.npy\")\n",
    "red_label = np.load(\"data-npy/red_label.npy\")\n",
    "\n",
    "# data normalization\n",
    "red_data -= red_data.mean(axis=0)\n",
    "red_data /= red_data.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle = np.arange(red_data.shape[0])\n",
    "np.random.shuffle(shuffle)\n",
    "\n",
    "remain_data = red_data[shuffle[:1400]]\n",
    "remain_label = red_label[shuffle[:1400]]\n",
    "\n",
    "test_data = red_data[shuffle[1400:]]\n",
    "test_label = red_label[shuffle[1400:]]\n",
    "\n",
    "train_data = remain_data[:1200]\n",
    "train_label = remain_label[:1200]\n",
    "\n",
    "valid_data = remain_data[1200:]\n",
    "valid_label = remain_label[1200:]\n",
    "\n",
    "redWineNN = DogikoLearn()\n",
    "redWineNN.rs_extend_regularizer(0.001, 1.)\n",
    "redWineNN.set_training_data(train_data, train_label)\n",
    "redWineNN.set_validation_data(valid_data, valid_label)\n",
    "redWineNN.add_layer(Layer(20, Hypertan()))\n",
    "redWineNN.add_layer(Layer(1, Identity()))\n",
    "redWineNN.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13, 0.411523904949 0.707362914669\n",
      "13, 0.428696094836 0.710178502728\n",
      "13, 0.423976369011 0.718064521873\n",
      "13, 0.423079443147 0.720443238911\n",
      "14, 0.411864300809 0.757808926239\n",
      "13, 0.422293709147 0.753596861498\n",
      "13, 0.421199293749 0.720192720042\n",
      "13, 0.43040671621 0.720056179822\n",
      "12, 0.432609218246 0.749111934873\n",
      "12, 0.425622334978 0.719732961138\n",
      "12, 0.426403938206 0.707289475885\n",
      "12, 0.42592479783 0.698895318695\n",
      "12, 0.433087970801 0.692245494707\n",
      "13, 0.410239668063 0.711015983286\n",
      "12, 0.455881751454 0.619673518931\n",
      "12, 0.444769697366 0.624597312284\n",
      "13, 0.422800752455 0.672444273025\n",
      "13, 0.415948498382 0.678852559244\n",
      "13, 0.412462041986 0.662371544064\n",
      "13, 0.417189332692 0.672372290995\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for t in range(20):\n",
    "    for l in range(redWineNN.ln-1):\n",
    "        redWineNN.neuron_proliferate(proliferating_layer=l,\n",
    "                                     proliferating_n=1,\n",
    "                                     output_weight_bound=0.001/np.sqrt(redWineNN.ly[l+1].nn)\n",
    "                                    )\n",
    "        redWineNN.train(1000, descent_method=\"Rprop\", termination=[10,30,0.])\n",
    "        redWineNN.neuron_refined(l,\n",
    "                                 reference_data=remain_data,\n",
    "                                 threshold=redWineNN.dimension()/(1200*redWineNN.ly[l].nn)\n",
    "                                )\n",
    "        redWineNN.train(1000, descent_method=\"Rprop\", termination=[10,30,0.])\n",
    "        print(redWineNN.ly[l].nn, end=\", \")\n",
    "    \n",
    "    print(redWineNN.training_error(),\n",
    "          redWineNN.validation_error()\n",
    "         )\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "|NN output - label| < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.688333333333\n",
      "0.555\n",
      "0.552763819095\n"
     ]
    }
   ],
   "source": [
    "print((np.abs(redWineNN.prediction(redWineNN.tx) - redWineNN.ty) < 0.5).mean())\n",
    "print((np.abs(redWineNN.prediction(redWineNN.vx) - redWineNN.vy) < 0.5).mean())\n",
    "print((np.abs(redWineNN.prediction(test_data) - test_label) < 0.5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy with class error be 1\n",
    "\n",
    "|NN output - label| < 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989166666667\n",
      "0.97\n",
      "0.969849246231\n"
     ]
    }
   ],
   "source": [
    "print((np.abs(redWineNN.prediction(redWineNN.tx) - redWineNN.ty) < 1.5).mean())\n",
    "print((np.abs(redWineNN.prediction(redWineNN.vx) - redWineNN.vy) < 1.5).mean())\n",
    "print((np.abs(redWineNN.prediction(test_data) - test_label) < 1.5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# White wine regression\n",
    "\n",
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "white_data = np.load(\"data-npy/white_data.npy\")\n",
    "white_label = np.load(\"data-npy/white_label.npy\")\n",
    "\n",
    "# data normalization\n",
    "white_data -= white_data.mean(axis=0)\n",
    "white_data /= white_data.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle = np.arange(white_data.shape[0])\n",
    "np.random.shuffle(shuffle)\n",
    "\n",
    "remain_data = white_data[shuffle[:4400]]\n",
    "remain_label = white_label[shuffle[:4400]]\n",
    "\n",
    "test_data = white_data[shuffle[4400:]]\n",
    "test_label = white_label[shuffle[4400:]]\n",
    "\n",
    "train_data = remain_data[:4000]\n",
    "train_label = remain_label[:4000]\n",
    "\n",
    "valid_data = remain_data[4000:]\n",
    "valid_label = remain_label[4000:]\n",
    "\n",
    "whiteWineNN = DogikoLearn()\n",
    "whiteWineNN.rs_extend_regularizer(0.001, 2.)\n",
    "whiteWineNN.set_training_data(train_data, train_label)\n",
    "whiteWineNN.set_validation_data(valid_data, valid_label)\n",
    "whiteWineNN.add_layer(Layer(20, Hypertan()))\n",
    "whiteWineNN.add_layer(Layer(1, Identity()))\n",
    "whiteWineNN.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17, 0.565374772286 0.775383954154\n",
      "17, 0.554441850755 0.798179423374\n",
      "18, 0.535386085769 0.773691814847\n",
      "19, 0.526733384269 0.76832836018\n",
      "20, 0.512601509179 0.752806677428\n",
      "21, 0.504511256895 0.757906683063\n",
      "22, 0.498292927449 0.771791930213\n",
      "23, 0.49069125304 0.767611027969\n",
      "24, 0.480538908919 0.763170414059\n",
      "24, 0.493811806686 0.739617039477\n",
      "25, 0.487242843478 0.752467306716\n",
      "25, 0.4886206371 0.760646429606\n",
      "25, 0.478655031049 0.736310350517\n",
      "26, 0.474104946903 0.721368009874\n",
      "27, 0.465239842137 0.724280420143\n",
      "28, 0.471582050546 0.729798417058\n",
      "28, 0.475427418888 0.773242830019\n",
      "28, 0.490719318172 0.746732525634\n",
      "29, 0.47296348248 0.742960992998\n",
      "29, 0.469475742323 0.751554005638\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for t in range(20):\n",
    "    for l in range(redWineNN.ln-1):\n",
    "        whiteWineNN.neuron_proliferate(proliferating_layer=l,\n",
    "                                     proliferating_n=1,\n",
    "                                     output_weight_bound=0.001/np.sqrt(whiteWineNN.ly[l+1].nn)\n",
    "                                    )\n",
    "        whiteWineNN.train(1000, descent_method=\"Rprop\", termination=[10,30,0.])\n",
    "        whiteWineNN.neuron_refined(l,\n",
    "                                 reference_data=remain_data,\n",
    "                                 threshold=whiteWineNN.dimension()/(4000*whiteWineNN.ly[l].nn)\n",
    "                                )\n",
    "        whiteWineNN.train(1000, descent_method=\"Rprop\", termination=[10,30,0.])\n",
    "        print(whiteWineNN.ly[l].nn, end=\", \")\n",
    "    \n",
    "    print(whiteWineNN.training_error(),\n",
    "          whiteWineNN.validation_error()\n",
    "         )\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "|NN output - label| < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6125\n",
      "0.5025\n",
      "0.512048192771\n"
     ]
    }
   ],
   "source": [
    "print((np.abs(whiteWineNN.prediction(whiteWineNN.tx) - whiteWineNN.ty) < 0.5).mean())\n",
    "print((np.abs(whiteWineNN.prediction(whiteWineNN.vx) - whiteWineNN.vy) < 0.5).mean())\n",
    "print((np.abs(whiteWineNN.prediction(test_data) - test_label) < 0.5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy with class error be 1\n",
    "\n",
    "|NN output - label| < 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.981\n",
      "0.9625\n",
      "0.957831325301\n"
     ]
    }
   ],
   "source": [
    "print((np.abs(whiteWineNN.prediction(whiteWineNN.tx) - whiteWineNN.ty) < 1.5).mean())\n",
    "print((np.abs(whiteWineNN.prediction(whiteWineNN.vx) - whiteWineNN.vy) < 1.5).mean())\n",
    "print((np.abs(whiteWineNN.prediction(test_data) - test_label) < 1.5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
